{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMXHpenvFRxmCYBo/0qInUB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DAaXXcVEws71","executionInfo":{"status":"ok","timestamp":1706859647419,"user_tz":-540,"elapsed":24182,"user":{"displayName":"Geass J","userId":"06622059162331348427"}},"outputId":"a13ab1cf-e082-48e2-e07c-076c1fede591"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6UIdipHBwRWV","executionInfo":{"status":"ok","timestamp":1706860079413,"user_tz":-540,"elapsed":6435,"user":{"displayName":"Geass J","userId":"06622059162331348427"}},"outputId":"864edbc0-cd6e-49f7-a335-5ad482f69206"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","10/10 [==============================] - 1s 19ms/step - loss: 0.6502 - accuracy: 0.6633 - val_loss: 0.6284 - val_accuracy: 0.7500\n","Epoch 2/100\n","10/10 [==============================] - 0s 5ms/step - loss: 0.6274 - accuracy: 0.7500 - val_loss: 0.6043 - val_accuracy: 0.7895\n","Epoch 3/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.6065 - accuracy: 0.7900 - val_loss: 0.5839 - val_accuracy: 0.8289\n","Epoch 4/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.5890 - accuracy: 0.8167 - val_loss: 0.5653 - val_accuracy: 0.8684\n","Epoch 5/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.5732 - accuracy: 0.8333 - val_loss: 0.5477 - val_accuracy: 0.8947\n","Epoch 6/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.5583 - accuracy: 0.8500 - val_loss: 0.5318 - val_accuracy: 0.9079\n","Epoch 7/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.5448 - accuracy: 0.8500 - val_loss: 0.5180 - val_accuracy: 0.9079\n","Epoch 8/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.5332 - accuracy: 0.8500 - val_loss: 0.5050 - val_accuracy: 0.9079\n","Epoch 9/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.5220 - accuracy: 0.8500 - val_loss: 0.4931 - val_accuracy: 0.9079\n","Epoch 10/100\n","10/10 [==============================] - 0s 7ms/step - loss: 0.5119 - accuracy: 0.8533 - val_loss: 0.4826 - val_accuracy: 0.9079\n","Epoch 11/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.5030 - accuracy: 0.8533 - val_loss: 0.4724 - val_accuracy: 0.9079\n","Epoch 12/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4946 - accuracy: 0.8533 - val_loss: 0.4634 - val_accuracy: 0.9079\n","Epoch 13/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4873 - accuracy: 0.8533 - val_loss: 0.4552 - val_accuracy: 0.9079\n","Epoch 14/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4806 - accuracy: 0.8533 - val_loss: 0.4476 - val_accuracy: 0.9079\n","Epoch 15/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4745 - accuracy: 0.8533 - val_loss: 0.4408 - val_accuracy: 0.9079\n","Epoch 16/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4690 - accuracy: 0.8533 - val_loss: 0.4354 - val_accuracy: 0.9079\n","Epoch 17/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4646 - accuracy: 0.8533 - val_loss: 0.4295 - val_accuracy: 0.9079\n","Epoch 18/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4600 - accuracy: 0.8533 - val_loss: 0.4240 - val_accuracy: 0.9079\n","Epoch 19/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4558 - accuracy: 0.8533 - val_loss: 0.4202 - val_accuracy: 0.9079\n","Epoch 20/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4524 - accuracy: 0.8533 - val_loss: 0.4160 - val_accuracy: 0.9079\n","Epoch 21/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4490 - accuracy: 0.8533 - val_loss: 0.4123 - val_accuracy: 0.9079\n","Epoch 22/100\n","10/10 [==============================] - 0s 5ms/step - loss: 0.4461 - accuracy: 0.8533 - val_loss: 0.4093 - val_accuracy: 0.9079\n","Epoch 23/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4436 - accuracy: 0.8533 - val_loss: 0.4062 - val_accuracy: 0.9079\n","Epoch 24/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4407 - accuracy: 0.8533 - val_loss: 0.4030 - val_accuracy: 0.9079\n","Epoch 25/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4383 - accuracy: 0.8533 - val_loss: 0.4017 - val_accuracy: 0.9079\n","Epoch 26/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4364 - accuracy: 0.8533 - val_loss: 0.3991 - val_accuracy: 0.9079\n","Epoch 27/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4340 - accuracy: 0.8533 - val_loss: 0.3971 - val_accuracy: 0.9079\n","Epoch 28/100\n","10/10 [==============================] - 0s 5ms/step - loss: 0.4322 - accuracy: 0.8533 - val_loss: 0.3948 - val_accuracy: 0.9079\n","Epoch 29/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4301 - accuracy: 0.8533 - val_loss: 0.3927 - val_accuracy: 0.9079\n","Epoch 30/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4283 - accuracy: 0.8533 - val_loss: 0.3910 - val_accuracy: 0.9079\n","Epoch 31/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4266 - accuracy: 0.8533 - val_loss: 0.3896 - val_accuracy: 0.9079\n","Epoch 32/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4249 - accuracy: 0.8533 - val_loss: 0.3883 - val_accuracy: 0.9079\n","Epoch 33/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4235 - accuracy: 0.8533 - val_loss: 0.3868 - val_accuracy: 0.9079\n","Epoch 34/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4221 - accuracy: 0.8533 - val_loss: 0.3854 - val_accuracy: 0.9079\n","Epoch 35/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4207 - accuracy: 0.8533 - val_loss: 0.3843 - val_accuracy: 0.9079\n","Epoch 36/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4193 - accuracy: 0.8533 - val_loss: 0.3838 - val_accuracy: 0.9079\n","Epoch 37/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4181 - accuracy: 0.8533 - val_loss: 0.3826 - val_accuracy: 0.9079\n","Epoch 38/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4169 - accuracy: 0.8533 - val_loss: 0.3821 - val_accuracy: 0.9079\n","Epoch 39/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4155 - accuracy: 0.8533 - val_loss: 0.3811 - val_accuracy: 0.9079\n","Epoch 40/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4144 - accuracy: 0.8533 - val_loss: 0.3803 - val_accuracy: 0.9079\n","Epoch 41/100\n","10/10 [==============================] - 0s 5ms/step - loss: 0.4132 - accuracy: 0.8533 - val_loss: 0.3796 - val_accuracy: 0.9079\n","Epoch 42/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4121 - accuracy: 0.8533 - val_loss: 0.3789 - val_accuracy: 0.9079\n","Epoch 43/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4111 - accuracy: 0.8533 - val_loss: 0.3785 - val_accuracy: 0.9079\n","Epoch 44/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4100 - accuracy: 0.8533 - val_loss: 0.3781 - val_accuracy: 0.9079\n","Epoch 45/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4092 - accuracy: 0.8533 - val_loss: 0.3777 - val_accuracy: 0.9079\n","Epoch 46/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4084 - accuracy: 0.8533 - val_loss: 0.3769 - val_accuracy: 0.9079\n","Epoch 47/100\n","10/10 [==============================] - 0s 5ms/step - loss: 0.4076 - accuracy: 0.8533 - val_loss: 0.3766 - val_accuracy: 0.9079\n","Epoch 48/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4068 - accuracy: 0.8533 - val_loss: 0.3764 - val_accuracy: 0.9079\n","Epoch 49/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4062 - accuracy: 0.8533 - val_loss: 0.3761 - val_accuracy: 0.9079\n","Epoch 50/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4055 - accuracy: 0.8533 - val_loss: 0.3759 - val_accuracy: 0.9079\n","Epoch 51/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4049 - accuracy: 0.8533 - val_loss: 0.3758 - val_accuracy: 0.9079\n","Epoch 52/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4042 - accuracy: 0.8533 - val_loss: 0.3756 - val_accuracy: 0.9079\n","Epoch 53/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4036 - accuracy: 0.8533 - val_loss: 0.3756 - val_accuracy: 0.9079\n","Epoch 54/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4030 - accuracy: 0.8533 - val_loss: 0.3756 - val_accuracy: 0.9079\n","Epoch 55/100\n","10/10 [==============================] - 0s 5ms/step - loss: 0.4024 - accuracy: 0.8533 - val_loss: 0.3756 - val_accuracy: 0.9079\n","Epoch 56/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.4018 - accuracy: 0.8533 - val_loss: 0.3755 - val_accuracy: 0.9079\n","Epoch 57/100\n","10/10 [==============================] - 0s 5ms/step - loss: 0.4012 - accuracy: 0.8533 - val_loss: 0.3756 - val_accuracy: 0.9079\n","Epoch 58/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4006 - accuracy: 0.8533 - val_loss: 0.3756 - val_accuracy: 0.9079\n","Epoch 59/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.4000 - accuracy: 0.8533 - val_loss: 0.3754 - val_accuracy: 0.9079\n","Epoch 60/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3994 - accuracy: 0.8533 - val_loss: 0.3751 - val_accuracy: 0.9079\n","Epoch 61/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3988 - accuracy: 0.8533 - val_loss: 0.3750 - val_accuracy: 0.9079\n","Epoch 62/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3981 - accuracy: 0.8533 - val_loss: 0.3747 - val_accuracy: 0.9079\n","Epoch 63/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3976 - accuracy: 0.8533 - val_loss: 0.3753 - val_accuracy: 0.9079\n","Epoch 64/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3970 - accuracy: 0.8533 - val_loss: 0.3754 - val_accuracy: 0.9079\n","Epoch 65/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3964 - accuracy: 0.8533 - val_loss: 0.3754 - val_accuracy: 0.9079\n","Epoch 66/100\n","10/10 [==============================] - 0s 5ms/step - loss: 0.3958 - accuracy: 0.8533 - val_loss: 0.3754 - val_accuracy: 0.9079\n","Epoch 67/100\n","10/10 [==============================] - 0s 5ms/step - loss: 0.3953 - accuracy: 0.8533 - val_loss: 0.3756 - val_accuracy: 0.9079\n","Epoch 68/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3946 - accuracy: 0.8533 - val_loss: 0.3756 - val_accuracy: 0.9079\n","Epoch 69/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3941 - accuracy: 0.8533 - val_loss: 0.3757 - val_accuracy: 0.9079\n","Epoch 70/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3935 - accuracy: 0.8533 - val_loss: 0.3761 - val_accuracy: 0.9079\n","Epoch 71/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3929 - accuracy: 0.8533 - val_loss: 0.3760 - val_accuracy: 0.9079\n","Epoch 72/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3924 - accuracy: 0.8533 - val_loss: 0.3758 - val_accuracy: 0.9079\n","Epoch 73/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8533 - val_loss: 0.3757 - val_accuracy: 0.9079\n","Epoch 74/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3912 - accuracy: 0.8533 - val_loss: 0.3762 - val_accuracy: 0.9079\n","Epoch 75/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3908 - accuracy: 0.8533 - val_loss: 0.3760 - val_accuracy: 0.9079\n","Epoch 76/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3902 - accuracy: 0.8533 - val_loss: 0.3758 - val_accuracy: 0.9079\n","Epoch 77/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3898 - accuracy: 0.8533 - val_loss: 0.3763 - val_accuracy: 0.9079\n","Epoch 78/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3891 - accuracy: 0.8533 - val_loss: 0.3763 - val_accuracy: 0.9079\n","Epoch 79/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3886 - accuracy: 0.8533 - val_loss: 0.3765 - val_accuracy: 0.9079\n","Epoch 80/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3880 - accuracy: 0.8533 - val_loss: 0.3766 - val_accuracy: 0.9079\n","Epoch 81/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3875 - accuracy: 0.8533 - val_loss: 0.3766 - val_accuracy: 0.9079\n","Epoch 82/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3870 - accuracy: 0.8533 - val_loss: 0.3768 - val_accuracy: 0.9079\n","Epoch 83/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3865 - accuracy: 0.8533 - val_loss: 0.3771 - val_accuracy: 0.9079\n","Epoch 84/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3858 - accuracy: 0.8533 - val_loss: 0.3769 - val_accuracy: 0.9079\n","Epoch 85/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3853 - accuracy: 0.8533 - val_loss: 0.3768 - val_accuracy: 0.9079\n","Epoch 86/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3847 - accuracy: 0.8533 - val_loss: 0.3771 - val_accuracy: 0.9079\n","Epoch 87/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3843 - accuracy: 0.8533 - val_loss: 0.3771 - val_accuracy: 0.9079\n","Epoch 88/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3837 - accuracy: 0.8533 - val_loss: 0.3771 - val_accuracy: 0.9079\n","Epoch 89/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3832 - accuracy: 0.8533 - val_loss: 0.3771 - val_accuracy: 0.9079\n","Epoch 90/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3827 - accuracy: 0.8533 - val_loss: 0.3772 - val_accuracy: 0.9079\n","Epoch 91/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3821 - accuracy: 0.8533 - val_loss: 0.3775 - val_accuracy: 0.9079\n","Epoch 92/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3817 - accuracy: 0.8533 - val_loss: 0.3777 - val_accuracy: 0.9079\n","Epoch 93/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3811 - accuracy: 0.8533 - val_loss: 0.3780 - val_accuracy: 0.9079\n","Epoch 94/100\n","10/10 [==============================] - 0s 6ms/step - loss: 0.3805 - accuracy: 0.8533 - val_loss: 0.3782 - val_accuracy: 0.9079\n","Epoch 95/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3799 - accuracy: 0.8533 - val_loss: 0.3783 - val_accuracy: 0.9079\n","Epoch 96/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3794 - accuracy: 0.8533 - val_loss: 0.3785 - val_accuracy: 0.9079\n","Epoch 97/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3789 - accuracy: 0.8533 - val_loss: 0.3790 - val_accuracy: 0.9079\n","Epoch 98/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3784 - accuracy: 0.8533 - val_loss: 0.3790 - val_accuracy: 0.9079\n","Epoch 99/100\n","10/10 [==============================] - 0s 4ms/step - loss: 0.3778 - accuracy: 0.8533 - val_loss: 0.3791 - val_accuracy: 0.9079\n","Epoch 100/100\n","10/10 [==============================] - 0s 5ms/step - loss: 0.3773 - accuracy: 0.8533 - val_loss: 0.3799 - val_accuracy: 0.9079\n","3/3 [==============================] - 0s 4ms/step - loss: 0.5124 - accuracy: 0.7979\n","Test Loss: 0.5124397873878479, Test Accuracy: 0.7978723645210266\n"]}],"source":["# Q2_0202. 모델의 목적은 환자의 병력을 기반으로 수술 후 1년 후 결과(사망 또는 생존)를 예측하세요.(학습 Only)\n","\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from sklearn.preprocessing import StandardScaler\n","\n","# 데이터셋 불러오기\n","data_set = np.loadtxt('/content/drive/MyDrive/kdt_jyg/workspace/m6_딥러닝/ThoraricSurgery3.csv', delimiter=\",\")\n","X = data_set[:, :16]\n","y = data_set[:, 16]\n","\n","# 특성 스케일링\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# 데이터를 훈련 및 테스트 세트로 분할 (80% 훈련, 20% 테스트)\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# 모델 구축\n","model = Sequential()\n","model.add(Dense(units=16, input_dim=16, activation='relu'))  # 은닉층 추가\n","model.add(Dense(units=8, activation='relu'))  # 은닉층 추가\n","model.add(Dense(units=1, activation='sigmoid'))  # 출력 레이어\n","\n","# 모델 컴파일\n","# 결과값이 0 또는 1이므로 이진데이터 분류\n","model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# 모델 훈련\n","model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n","\n","# 테스트\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"]}]}